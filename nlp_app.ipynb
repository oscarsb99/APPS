{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqyyYbzJ3lId",
        "outputId": "e1e018ec-feb9-4d6d-9e98-8b62c5607f03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting nlp_app.py\n"
          ]
        }
      ],
      "source": [
        "! pip install streamlit\n",
        "! pip install pyngrok\n",
        "! pip install emoji\n",
        "! pip install demoji\n",
        "! pip install transformers\n",
        "! pip install google-api-python-client\n",
        "\n",
        "%%writefile nlp_app.py\n",
        "\n",
        "import streamlit as st\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from wordcloud import WordCloud\n",
        "import emoji\n",
        "import demoji\n",
        "from transformers import pipeline\n",
        "from googleapiclient.discovery import build\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Set primary color\n",
        "primary_color = '#CDCDCD'  # Replace with your desired color in hexadecimal format\n",
        "\n",
        "# Set background color\n",
        "background_color = '#CDCDCD'  # Replace with your desired color in hexadecimal format\n",
        "\n",
        "# Set the overall theme\n",
        "st.set_page_config(\n",
        "    page_title=\"Youtube NLP App\",\n",
        "    page_icon=\"ðŸ“Š\",\n",
        "    layout=\"centered\",\n",
        "    initial_sidebar_state=\"auto\",\n",
        ")\n",
        "\n",
        "# Apply the custom theme\n",
        "css = f\"\"\"\n",
        "    body {{\n",
        "        color: {primary_color};\n",
        "        background-color: {background_color};\n",
        "    }}\n",
        "\n",
        "    .css-1l02zno {{\n",
        "        color: {primary_color};\n",
        "    }}\n",
        "\"\"\"\n",
        "st.markdown(f'<style>{css}</style>', unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "stopw = stopwords.words('english')\n",
        "\n",
        "# Function to extract the video ID\n",
        "def extract_video_id(url):\n",
        "    # Extract the video ID using the pattern\n",
        "    match = re.search(r\"watch\\?v=([a-zA-Z0-9_-]{11})\", url)\n",
        "    if match:\n",
        "        video_id = match.group(1)\n",
        "        return video_id\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Function to categorize the sentiment\n",
        "def categorize_sentiment(text):\n",
        "    # Create an instance of the SentimentIntensityAnalyzer\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "    # Analyze sentiment\n",
        "    sentiment_scores = sid.polarity_scores(text)\n",
        "\n",
        "    # Categorize sentiment\n",
        "    if sentiment_scores['pos'] > sentiment_scores['neg']:\n",
        "        return 'Positive'\n",
        "    else:\n",
        "        return 'Negative'\n",
        "\n",
        "# Function to preprocess comments and generate bag of words\n",
        "def bag_of_words(data):\n",
        "    bag_words = [a.lower() for i in data for a in re.split(' ', str(i)) if a.lower() not in stopw and len(a) > 3]\n",
        "    return bag_words\n",
        "\n",
        "# Define the Streamlit app\n",
        "st.title(\"YouTube Comments Analysis ðŸ“Š\")\n",
        "st.write(\"Welcome to the *YouTube Comments Analysis App!* (to get the methodology explore the final section).\")\n",
        "\n",
        "# YouTube URL input\n",
        "url = st.text_input(\"Enter YouTube video URL (e.g., https://www.youtube.com/watch?v=JvEas_zZ4fM)\")\n",
        "\n",
        "if st.button(\"Analyze\"):\n",
        "    # Extract the video ID\n",
        "    video_id = extract_video_id(url)\n",
        "    if video_id is None:\n",
        "        st.error(\"Invalid YouTube URL. Please enter a valid URL.\")\n",
        "        st.stop()\n",
        "\n",
        "    # API\n",
        "    api_key = \"AIzaSyBUajBi65Ou3NlU2hDr-XwhnHj8G4AwjJk\"  # Replace with your actual API key\n",
        "    youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
        "    comments = []\n",
        "\n",
        "    request = youtube.commentThreads().list(\n",
        "        part=\"snippet\",\n",
        "        videoId=video_id,\n",
        "        maxResults=100,  # Maximum number of comments to retrieve per page\n",
        "        textFormat=\"plainText\"\n",
        "    )\n",
        "\n",
        "    while request:\n",
        "        response = request.execute()\n",
        "\n",
        "        for item in response[\"items\"]:\n",
        "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "            comments.append(comment)\n",
        "\n",
        "        request = youtube.commentThreads().list_next(request, response)\n",
        "\n",
        "    # Display comments\n",
        "    st.subheader(\"Comments (first 10)\")\n",
        "    st.write(comments[:10])\n",
        "\n",
        "    # Emojis information\n",
        "    emojis = [emoji.emoji_list(i)[0]['emoji'] for i in comments if len(emoji.emoji_list(i)) > 0]\n",
        "    emoji_name = [emoji.demojize(i) for i in emojis]\n",
        "    df_emojis = pd.DataFrame({'emojis': emojis, 'emoji_name': emoji_name})\n",
        "    df_emojis = pd.DataFrame(df_emojis.groupby(['emojis']).count()).reset_index()\n",
        "    df_emojis.columns = ['emojis', 'values']\n",
        "    df_emojis = df_emojis.sort_values('values', ascending=False).reset_index()[['emojis', 'values']]\n",
        "\n",
        "    # Display emojis information\n",
        "    st.subheader(\"Emojis Information\")\n",
        "    st.write(df_emojis.head())\n",
        "\n",
        "    # Bag of words\n",
        "    bag_words = bag_of_words(comments)\n",
        "    fdist = nltk.FreqDist(bag_words)\n",
        "    keys = fdist.keys()\n",
        "    values = fdist.values()\n",
        "\n",
        "    df_words = pd.DataFrame({'keys': keys, 'values': values}).sort_values(['values'], ascending=False).reset_index()[\n",
        "        ['keys', 'values']]\n",
        "\n",
        "    # Display word frequency\n",
        "    st.subheader(\"Word Frequency\")\n",
        "    st.write(df_words.head(5))\n",
        "\n",
        "    # Plot word frequency\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    plt.style.use('seaborn-white')\n",
        "    plt.barh(df_words['keys'][:15], df_words['values'][:15], color='SlateBlue')\n",
        "    plt.title('Word Frequency')\n",
        "    ax.invert_yaxis()\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # Wordcloud\n",
        "    bag_of_words_str = \" \".join(bag_words)\n",
        "    wc = WordCloud(\n",
        "        background_color='white',\n",
        "        stopwords=stopwords.words('english'),\n",
        "        width=3000,\n",
        "        height=2000,\n",
        "        collocations=False,\n",
        "        colormap='Dark2',\n",
        "        max_words=40\n",
        "    )\n",
        "    wc.generate(bag_of_words_str)\n",
        "\n",
        "    # Display wordcloud\n",
        "    st.subheader(\"Wordcloud\")\n",
        "    plt.figure(figsize=[7, 7])\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    st.pyplot(plt)\n",
        "\n",
        "    # Sentiment analysis\n",
        "    df = pd.DataFrame(columns=['Sentence', 'Sentiment'])\n",
        "    for comment in comments:\n",
        "        sentiment = categorize_sentiment(comment)\n",
        "        df = df.append({'Sentence': comment, 'Sentiment': sentiment}, ignore_index=True)\n",
        "\n",
        "    # Sentiment distribution\n",
        "    positive_count = df['Sentiment'].value_counts()['Positive']\n",
        "    negative_count = df['Sentiment'].value_counts()['Negative']\n",
        "    labels = ['Positive', 'Negative']\n",
        "    sizes = [positive_count, negative_count]\n",
        "    colors = ['green', 'red']\n",
        "\n",
        "    # Display sentiment distribution\n",
        "    st.subheader(\"Sentiment Distribution\")\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "    plt.axis('equal')\n",
        "    plt.title('Sentiment Distribution')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "st.write(\"Bonus: To get easly a url look at this table.\")\n",
        "\n",
        "def display_company_table():\n",
        "    # Define the company data\n",
        "    company_data = {\n",
        "        'Video Name': ['Nassim Nicholas Taleb: How to Live in a World we Dont Understand','Chomsky-Foucault Debate on Power vs Justice (1971)',\n",
        "        '15 Minutes of Kevin Hart Dad Jokes | Netflix Is A Joke','ChatGPT Tutorial: How to Use Chat GPT'],\n",
        "        'URL': ['https://www.youtube.com/watch?v=iEnmjMgP_Jo&t=63s','https://www.youtube.com/watch?v=xpVQ3l5P0A4',\n",
        "        'https://www.youtube.com/watch?v=gbxSpLDQehg','https://www.youtube.com/watch?v=Gaf_jCnA6mc']\n",
        "    }\n",
        "\n",
        "    # Create a DataFrame from the company data\n",
        "    df = pd.DataFrame(company_data)\n",
        "\n",
        "    # Display the DataFrame as a table in Streamlit\n",
        "    st.table(df)\n",
        "\n",
        "# Run the function to display the company table\n",
        "display_company_table()\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.write(\"The methodology summarized is:\")\n",
        "st.write(\"- The app was developed using python, streamlit and NLP techniques.\")\n",
        "st.write(\"- The app leverages the YouTube Data API to fetch comments related to the provided video ID.\")\n",
        "st.write(\"- With NLTK and emoji library the word-emoji frequency is getted.\")\n",
        "st.write(\"- The Sentiment Analysis is done with VADER model.\")\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"Created by Oscar Gomez - oscar.gomezr0414@gmail.com\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.3 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "ab7e3eb079da94abf84898a669a2193b4bbe15555a6541f21cdabf6e065d0e54"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
